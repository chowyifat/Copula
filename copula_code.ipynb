{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Computing\n",
    "import tensorflow as tf\n",
    "devices=tf.config.list_physical_devices()\n",
    "tf.config.set_visible_devices([],'GPU')\n",
    "tf.config.set_visible_devices(devices[0],'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market=pd.read_csv('market.csv')\n",
    "stock=pd.read_csv('stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock return from unit to %\n",
    "stock['rret']=stock['rret']*100\n",
    "# mktret return \n",
    "market['mktret']=market['mktrf']+market['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market['date']=pd.to_datetime(market['date'])\n",
    "stock['date']=pd.to_datetime(stock['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock=stock[stock['date']>='1995-01']\n",
    "market=market[market['date']>='1995-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop stocks traded less than 30 days\n",
    "stock_list=[]\n",
    "for i,group in stock.groupby('permno'):\n",
    "    if group.shape[0]<30:\n",
    "        continue\n",
    "    else:\n",
    "        stock_list.append(group)\n",
    "stock=pd.concat(stock_list,axis=0)\n",
    "stock.dropna(how='any',axis=0,inplace=True)\n",
    "stock.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the last day of the month \n",
    "monthly_stock_list=[]\n",
    "for i,group in stock.groupby('permno'):\n",
    "    group=group.resample('Q',on='date').last()\n",
    "    monthly_stock_list.append(group)\n",
    "monthly_stock=pd.concat(monthly_stock_list,axis=0)\n",
    "monthly_stock.dropna(how='any',axis=0,inplace=True)\n",
    "monthly_stock.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last day in the daily sample\n",
    "stock['mark']=0\n",
    "stock_list=[]\n",
    "for (i,group),(monthly_i,monthly_group) in zip(stock.groupby('permno'),monthly_stock.groupby('permno')):\n",
    "    judge=monthly_group['date'].unique()\n",
    "    group.loc[group['date'].isin(judge),'mark']=1\n",
    "    stock_list.append(group)\n",
    "stock=pd.concat(stock_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical CDF\n",
    "def CDF(x):\n",
    "    x_sorted=x.sort_values()\n",
    "\n",
    "    index=x_sorted.index\n",
    "    n=np.arange(len(x_sorted))\n",
    "    n=n+1\n",
    "    n=pd.Series(n,index=index,name='cdf_series')\n",
    "\n",
    "    cdf_series=n/(len(x_sorted)+1)\n",
    "\n",
    "    x=pd.concat([x,cdf_series],axis=1)\n",
    "    return x['cdf_series']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_market=CDF(market['mktret'])\n",
    "cdf_stock=CDF(stock['rret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market=pd.concat([market,cdf_market],axis=1)\n",
    "stock=pd.concat([stock,cdf_stock],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge factor to stock\n",
    "df=pd.merge(stock,market,on='date',how='left')\n",
    "df.rename(columns={'cdf_series_x':'cdf_stock','cdf_series_y':'cdf_market'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "df.to_csv('data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_train(group):\n",
    "    import tensorflow as tf\n",
    "    devices=tf.config.list_physical_devices()\n",
    "    tf.config.set_visible_devices([],'GPU')\n",
    "    tf.config.set_visible_devices(devices[0],'CPU')\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from scipy import integrate\n",
    "\n",
    "\n",
    "    # Clayton Copula lower tail dependence \n",
    "    # theta>=0\n",
    "    def Clayton_Copula_CDF(theta,u1,u2):\n",
    "        if theta==0:\n",
    "            return u1*u2\n",
    "        else:\n",
    "            base=tf.math.pow(u1,-theta)+tf.math.pow(u2,-theta)-1\n",
    "            return tf.math.pow(base,-1/theta)\n",
    "\n",
    "    def Clayton_Copula_PDF(theta,u1,u2):\n",
    "        return (theta + 1)/(u1**(theta + 1)*u2**(theta + 1)*(1/u1**theta + 1/u2**theta - 1)**(1/theta + 2))\n",
    "\n",
    "\n",
    "    # Caussian_Copula no tail dependence \n",
    "    # -1<=theta<=1\n",
    "    def Gaussian_Copula_CDF(theta,u1,u2):\n",
    "        u1_inv=stats.norm.ppf(u1)\n",
    "        u2_inv=stats.norm.ppf(u2)\n",
    "\n",
    "        def f(x,y,theta):\n",
    "            return 1/(2*np.pi*tf.math.sqrt(1-theta**2))*tf.math.pow(np.e,(-y*y+2*theta*x*y-x*x)/(2*(1-theta**2)))\n",
    "\n",
    "        def cdf(theta,u1_inv,u2_inv):\n",
    "            return integrate.dblquad(f,-np.inf,u1_inv,-np.inf,u2_inv,args=(theta,))\n",
    "\n",
    "        return cdf(theta,u1_inv,u2_inv)[0]\n",
    "\n",
    "    def Gaussian_Copula_PDF(theta,u1,u2):\n",
    "        u1_inv=stats.norm.ppf(u1)\n",
    "        u2_inv=stats.norm.ppf(u2)\n",
    "\n",
    "        def f(x,y,theta):\n",
    "            return 1/(2*np.pi*tf.math.sqrt(1-theta**2))*tf.math.exp((-x*x+2*theta*x*y-y*y)/(2*(1-theta**2)))\n",
    "\n",
    "        return 1/stats.norm.pdf(u1)*1/stats.norm.pdf(u2)*f(u1_inv,u2_inv,theta)\n",
    "\n",
    "\n",
    "    # Gumbel_Copula Copula upper tail dependence \n",
    "    # theta>=1\n",
    "    def Gumbel_Copula_CDF(theta,u1,u2):\n",
    "        m1=-tf.math.log(u1)\n",
    "        m2=-tf.math.log(u2)\n",
    "        return tf.math.exp(-tf.math.pow(m1**theta+m2**theta,1/theta))\n",
    "\n",
    "    def Gumbel_Copula_PDF(theta,u1,u2):\n",
    "        return (tf.math.exp(-((-tf.math.log(u1))**theta + (-tf.math.log(u2))**theta)**(1/theta))*(-tf.math.log(u1))**(theta - 1)*((-tf.math.log(u1))**theta + (-tf.math.log(u2))**theta)**(1/theta - 1))/u1\n",
    "\n",
    "    #combine copulas\n",
    "    def combine_Copula(w1,w2,w3,Copula_1,Copula_2,Copula_3):\n",
    "        return w1*Copula_1+w2*Copula_2+w3*Copula_3\n",
    "        \n",
    "    def mle(theta1,theta2,theta3,w1,w2,w3,u1,u2):\n",
    "        Copula_1=Clayton_Copula_PDF(theta1,u1,u2)\n",
    "        Copula_2=Gaussian_Copula_PDF(theta2,u1,u2)\n",
    "        Copula_3=Gumbel_Copula_PDF(theta3,u1,u2)\n",
    "        \n",
    "        prop_series=combine_Copula(w1,w2,w3,Copula_1,Copula_2,Copula_3)\n",
    "        prop_series=tf.math.log(prop_series)\n",
    "        result=tf.reduce_sum(prop_series)\n",
    "        return -result\n",
    "    \n",
    "    # theta>=0\n",
    "    def dClayton_du(theta,u1,u2):\n",
    "        return 1/(u1**(theta+1)*(1/u1**theta+1/u2**theta-1)**(1/theta+1))\n",
    "\n",
    "    # -1<=theta<=1\n",
    "    def dGaussian_du(theta,u1,u2):\n",
    "        u1_inv=stats.norm.ppf(u1)\n",
    "        u2_inv=stats.norm.ppf(u2)\n",
    "\n",
    "        def f(x,y,theta):\n",
    "            return 1/(2*np.pi*tf.math.sqrt(1-theta**2))*tf.math.pow(np.e,(-y*y+2*theta*x*y-x*x)/(2*(1-theta**2)))\n",
    "\n",
    "        def derivative(theta,u1_inv,u2_inv):\n",
    "            result=integrate.quad(f,-np.inf,u1_inv,args=(u2_inv,theta))\n",
    "            return result[0]\n",
    "\n",
    "        return 1/stats.norm.pdf(u2)*derivative(theta,u1_inv,u2_inv)\n",
    "\n",
    "    # theta>=1\n",
    "    def dGumbel_du(theta,u1,u2):\n",
    "        return (np.exp(-((-np.log(u1))**theta+(-np.log(u2))**theta)*(1/theta))*(-np.log(u1))**(theta-1)*((-np.log(u1))**theta+(-np.log(u2))**theta)**(1/theta-1))/u1\n",
    "\n",
    "    class GreaterEqualZero(tf.keras.constraints.Constraint):\n",
    "        def __call__(self,value):\n",
    "            value=tf.math.maximum(value,0)\n",
    "            return value\n",
    "\n",
    "    class MinusOneToOne(tf.keras.constraints.Constraint):\n",
    "        def __call__(self,value):\n",
    "            value=tf.math.maximum(value,-1)\n",
    "            value=tf.math.minimum(value,1)\n",
    "            return value\n",
    "\n",
    "    class ZeroToOne(tf.keras.constraints.Constraint):\n",
    "        def __call__(self,value):\n",
    "            value=tf.math.maximum(value,0)\n",
    "            value=tf.math.minimum(value,1)\n",
    "            return value\n",
    "\n",
    "    class GreaterEqualOne(tf.keras.constraints.Constraint):\n",
    "        def __call__(self,value):\n",
    "            value=tf.math.maximum(value,1)\n",
    "            return value\n",
    "\n",
    "    cons1=GreaterEqualZero()\n",
    "    cons2=MinusOneToOne()\n",
    "    cons3=GreaterEqualOne()\n",
    "    cons4=ZeroToOne()\n",
    "\n",
    "\n",
    "    first_index=group.head(1).index[0]\n",
    "    adam_attempt=0\n",
    "    adam_decay_rate_list=np.concatenate([np.arange(0.000,0.020,0.002),np.arange(0.02,0.20,0.02),np.arange(0.2,2.0,0.1),np.arange(2,20,1),np.arange(20,100,10),np.arange(100,1000,100)],axis=0)\n",
    "    \n",
    "    # define window CPU training\n",
    "    def window_train(window):   \n",
    "        nonlocal adam_attempt\n",
    "        try:\n",
    "            # construct maximum likelihood function \n",
    "            theta1=tf.Variable(np.random.uniform(0,1),name='theta1',dtype='float64',constraint=cons1)\n",
    "            theta2=tf.Variable(np.random.uniform(-1,1),name='theta2',dtype='float64',constraint=cons2)\n",
    "            theta3=tf.Variable(np.random.uniform(1,2),name='theta3',dtype='float64',constraint=cons3)\n",
    "            w1=tf.Variable(np.random.uniform(0,0.5),name='w1',dtype='float64',constraint=cons4)\n",
    "            w2=tf.Variable(np.random.uniform(0,0.5),name='w2',dtype='float64',constraint=cons4)\n",
    "            w3=tf.Variable(np.random.uniform(0,0.5),name='w3',dtype='float64',constraint=cons4)\n",
    "            beta=tf.constant(1e5,name='beta',dtype='float64')\n",
    "\n",
    "            cdf_stock=tf.convert_to_tensor(window['cdf_stock'].to_numpy(),dtype='float64')\n",
    "            cdf_market=tf.convert_to_tensor(window['cdf_market'].to_numpy(),dtype='float64')\n",
    "            train=tf.data.Dataset.from_tensor_slices((cdf_stock,cdf_market))\n",
    "            train=train.batch(window.shape[0])\n",
    "        \n",
    "            mle_list=[]\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=0.001,decay=adam_decay_rate_list[adam_attempt])\n",
    "            result_list=[]\n",
    "            for epoch in range(2000):\n",
    "                for cdf_stock,cdf_market in train:\n",
    "                    with tf.GradientTape(persistent=True) as tape:\n",
    "                        result=(\n",
    "                            mle(theta1,theta2,theta3,w1,w2,w3,cdf_stock,cdf_market)+\n",
    "                            beta*(w1+w2+w3-1)**2\n",
    "                        )\n",
    "                    result_list.append(result)\n",
    "                    gradients=tf.clip_by_norm(tape.gradient(result,(theta1,theta2,theta3,w1,w2,w3)),1)\n",
    "\n",
    "                    optimizer.apply_gradients(zip(gradients,(theta1,theta2,theta3,w1,w2,w3)))  \n",
    "                    if np.isnan(result):\n",
    "                        raise\n",
    "                \n",
    "            # LTD and UTD\n",
    "            theta1,theta2,theta3,w1,w2,w3=np.round([theta1,theta2,theta3,w1,w2,w3],4)\n",
    "\n",
    "            u_low=tf.constant(1e-300,dtype='float64')\n",
    "            Copula_1_LTD=Clayton_Copula_CDF(theta1,u_low,u_low)/u_low\n",
    "            LTD=np.round(w1*Copula_1_LTD,4)\n",
    "\n",
    "            u_high=tf.constant(1-1e-16,dtype='float64')\n",
    "            Copula_3_UTD=2-dGumbel_du(theta3,u_high,u_high)\n",
    "            UTD=np.round(w3*Copula_3_UTD,4)\n",
    "\n",
    "            trained=1\n",
    "\n",
    "            adam_attempt=adam_attempt+1\n",
    "\n",
    "        except:\n",
    "            adam_attempt=adam_attempt+1\n",
    "            if adam_attempt<adam_decay_rate_list.shape[0]:\n",
    "                trained,LTD,UTD=window_train(window)\n",
    "            else:\n",
    "                trained,LTD,UTD=1,np.nan,np.nan \n",
    "        return trained,LTD,UTD\n",
    "    \n",
    "    \n",
    "    # apply row train\n",
    "    def row_by_window(row,group):\n",
    "        nonlocal first_index\n",
    "        index=row.name\n",
    "        mark=row['mark']\n",
    "\n",
    "        if mark==0:\n",
    "            return 0,np.nan,np.nan\n",
    "        elif (mark==1) & (index-251<first_index):\n",
    "            return 0,np.nan,np.nan\n",
    "        else:\n",
    "            window=group.loc[index-251:index,:]\n",
    "            trained,LTD,UTD=window_train(window)\n",
    "            return trained,LTD,UTD\n",
    "    \n",
    "    train=group.apply(row_by_window,axis=1,result_type='expand',args=(group,))\n",
    "    train.columns=['trained','LTD','UTD']\n",
    "    group=pd.concat([group,train],axis=1)\n",
    "    group=group[(group['mark']==1)&(group['trained']==1)]\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(nb_workers=60)\n",
    "final=df.groupby('permno').parallel_apply(group_by_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bfad08b73d8ae58273d7e252c9e1d922058e2a21d3a8ccb2a1b9808ae54a57f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
